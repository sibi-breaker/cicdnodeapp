
==> Audit <==
|------------|---------------------|----------|--------------|---------|---------------------|---------------------|
|  Command   |        Args         | Profile  |     User     | Version |     Start Time      |      End Time       |
|------------|---------------------|----------|--------------|---------|---------------------|---------------------|
| start      |                     | minikube | sibichandran | v1.35.0 | 28 Jan 25 09:51 UTC | 28 Jan 25 09:52 UTC |
| start      |                     | minikube | sibichandran | v1.35.0 | 28 Jan 25 10:07 UTC | 28 Jan 25 10:08 UTC |
| stop       |                     | minikube | sibichandran | v1.35.0 | 28 Jan 25 10:25 UTC | 28 Jan 25 10:25 UTC |
| start      |                     | minikube | sibichandran | v1.35.0 | 28 Jan 25 10:25 UTC | 28 Jan 25 10:25 UTC |
| service    | nodejs-app --url    | minikube | sibichandran | v1.35.0 | 28 Jan 25 11:00 UTC |                     |
| service    | nodejs-app --url    | minikube | sibichandran | v1.35.0 | 28 Jan 25 11:01 UTC |                     |
| docker-env | minikube docker-env | minikube | sibichandran | v1.35.0 | 28 Jan 25 11:04 UTC | 28 Jan 25 11:04 UTC |
| docker-env | minikube docker-env | minikube | sibichandran | v1.35.0 | 28 Jan 25 11:04 UTC | 28 Jan 25 11:04 UTC |
| service    | list                | minikube | sibichandran | v1.35.0 | 28 Jan 25 11:09 UTC | 28 Jan 25 11:09 UTC |
| service    | nodejs-app --url    | minikube | sibichandran | v1.35.0 | 28 Jan 25 11:09 UTC |                     |
|------------|---------------------|----------|--------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/01/28 10:25:20
Running on machine: Sibi
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0128 10:25:20.043722   80575 out.go:345] Setting OutFile to fd 1 ...
I0128 10:25:20.043813   80575 out.go:397] isatty.IsTerminal(1) = true
I0128 10:25:20.043815   80575 out.go:358] Setting ErrFile to fd 2...
I0128 10:25:20.043817   80575 out.go:397] isatty.IsTerminal(2) = true
I0128 10:25:20.043920   80575 root.go:338] Updating PATH: /home/sibichandran/.minikube/bin
W0128 10:25:20.043980   80575 root.go:314] Error reading config file at /home/sibichandran/.minikube/config/config.json: open /home/sibichandran/.minikube/config/config.json: no such file or directory
I0128 10:25:20.044159   80575 out.go:352] Setting JSON to false
I0128 10:25:20.044587   80575 start.go:129] hostinfo: {"hostname":"Sibi","uptime":15935,"bootTime":1738043985,"procs":41,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"5.15.167.4-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"3dfdb378-df7b-4f74-8882-8ce69e5fdffb"}
I0128 10:25:20.044614   80575 start.go:139] virtualization:  guest
I0128 10:25:20.046394   80575 out.go:177] 😄  minikube v1.35.0 on Ubuntu 24.04 (amd64)
I0128 10:25:20.049574   80575 notify.go:220] Checking for updates...
I0128 10:25:20.049638   80575 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0128 10:25:20.049687   80575 driver.go:394] Setting default libvirt URI to qemu:///system
I0128 10:25:20.068291   80575 docker.go:123] docker version: linux-26.1.3:
I0128 10:25:20.068373   80575 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0128 10:25:20.091403   80575 info.go:266] docker info: {ID:2d8c78a4-94b7-47d7-8a3f-a20a8d51830a Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:29 OomKillDisable:true NGoroutines:41 SystemTime:2025-01-28 10:25:20.080900072 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8144470016 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Sibi Labels:[] ExperimentalBuild:false ServerVersion:26.1.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-ai: no such file or directory Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-buildx: no such file or directory Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-compose: no such file or directory Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-debug: no such file or directory Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-desktop: no such file or directory Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-dev: no such file or directory Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-extension: no such file or directory Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-feedback: no such file or directory Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-init: no such file or directory Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-sbom: no such file or directory Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-scout: no such file or directory Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout]] Warnings:<nil>}}
I0128 10:25:20.091464   80575 docker.go:318] overlay module found
I0128 10:25:20.093785   80575 out.go:177] ✨  Using the docker driver based on existing profile
I0128 10:25:20.095703   80575 start.go:297] selected driver: docker
I0128 10:25:20.095708   80575 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sibichandran:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0128 10:25:20.095750   80575 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0128 10:25:20.095801   80575 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0128 10:25:20.116349   80575 info.go:266] docker info: {ID:2d8c78a4-94b7-47d7-8a3f-a20a8d51830a Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:29 OomKillDisable:true NGoroutines:41 SystemTime:2025-01-28 10:25:20.108918913 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Ubuntu 24.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8144470016 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Sibi Labels:[] ExperimentalBuild:false ServerVersion:26.1.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-ai: no such file or directory Name:ai Path:/usr/local/lib/docker/cli-plugins/docker-ai] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-buildx: no such file or directory Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-compose: no such file or directory Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-debug: no such file or directory Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-desktop: no such file or directory Name:desktop Path:/usr/local/lib/docker/cli-plugins/docker-desktop] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-dev: no such file or directory Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-extension: no such file or directory Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-feedback: no such file or directory Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-init: no such file or directory Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-sbom: no such file or directory Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom] map[Err:failed to fetch metadata: fork/exec /usr/local/lib/docker/cli-plugins/docker-scout: no such file or directory Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout]] Warnings:<nil>}}
I0128 10:25:20.117249   80575 cni.go:84] Creating CNI manager for ""
I0128 10:25:20.117290   80575 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 10:25:20.117333   80575 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sibichandran:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0128 10:25:20.119948   80575 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0128 10:25:20.122007   80575 cache.go:121] Beginning downloading kic base image for docker with docker
I0128 10:25:20.124769   80575 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0128 10:25:20.126692   80575 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0128 10:25:20.126720   80575 preload.go:146] Found local preload: /home/sibichandran/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0128 10:25:20.126725   80575 cache.go:56] Caching tarball of preloaded images
I0128 10:25:20.126799   80575 preload.go:172] Found /home/sibichandran/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0128 10:25:20.126803   80575 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0128 10:25:20.126862   80575 profile.go:143] Saving config to /home/sibichandran/.minikube/profiles/minikube/config.json ...
I0128 10:25:20.126889   80575 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0128 10:25:20.144616   80575 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0128 10:25:20.144625   80575 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0128 10:25:20.144643   80575 cache.go:227] Successfully downloaded all kic artifacts
I0128 10:25:20.144657   80575 start.go:360] acquireMachinesLock for minikube: {Name:mk022678155cfda77009c81c17eab13379eda9de Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0128 10:25:20.144697   80575 start.go:364] duration metric: took 31.292µs to acquireMachinesLock for "minikube"
I0128 10:25:20.144706   80575 start.go:96] Skipping create...Using existing machine configuration
I0128 10:25:20.144708   80575 fix.go:54] fixHost starting: 
I0128 10:25:20.144814   80575 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 10:25:20.158295   80575 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0128 10:25:20.158309   80575 fix.go:138] unexpected machine state, will restart: <nil>
I0128 10:25:20.160250   80575 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0128 10:25:20.162228   80575 cli_runner.go:164] Run: docker start minikube
I0128 10:25:20.654461   80575 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 10:25:20.670325   80575 kic.go:430] container "minikube" state is running.
I0128 10:25:20.670598   80575 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 10:25:20.682869   80575 profile.go:143] Saving config to /home/sibichandran/.minikube/profiles/minikube/config.json ...
I0128 10:25:20.683045   80575 machine.go:93] provisionDockerMachine start ...
I0128 10:25:20.683074   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:20.702635   80575 main.go:141] libmachine: Using SSH client type: native
I0128 10:25:20.702815   80575 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0128 10:25:20.702822   80575 main.go:141] libmachine: About to run SSH command:
hostname
I0128 10:25:20.703545   80575 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:50640->127.0.0.1:32787: read: connection reset by peer
I0128 10:25:23.832811   80575 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0128 10:25:23.832823   80575 ubuntu.go:169] provisioning hostname "minikube"
I0128 10:25:23.832861   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:23.846282   80575 main.go:141] libmachine: Using SSH client type: native
I0128 10:25:23.846444   80575 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0128 10:25:23.846451   80575 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0128 10:25:23.979249   80575 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0128 10:25:23.979302   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:23.998809   80575 main.go:141] libmachine: Using SSH client type: native
I0128 10:25:23.998965   80575 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0128 10:25:23.998977   80575 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0128 10:25:24.120403   80575 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 10:25:24.120414   80575 ubuntu.go:175] set auth options {CertDir:/home/sibichandran/.minikube CaCertPath:/home/sibichandran/.minikube/certs/ca.pem CaPrivateKeyPath:/home/sibichandran/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/sibichandran/.minikube/machines/server.pem ServerKeyPath:/home/sibichandran/.minikube/machines/server-key.pem ClientKeyPath:/home/sibichandran/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/sibichandran/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/sibichandran/.minikube}
I0128 10:25:24.120422   80575 ubuntu.go:177] setting up certificates
I0128 10:25:24.120427   80575 provision.go:84] configureAuth start
I0128 10:25:24.120454   80575 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 10:25:24.132535   80575 provision.go:143] copyHostCerts
I0128 10:25:24.132564   80575 exec_runner.go:144] found /home/sibichandran/.minikube/ca.pem, removing ...
I0128 10:25:24.132573   80575 exec_runner.go:203] rm: /home/sibichandran/.minikube/ca.pem
I0128 10:25:24.132640   80575 exec_runner.go:151] cp: /home/sibichandran/.minikube/certs/ca.pem --> /home/sibichandran/.minikube/ca.pem (1094 bytes)
I0128 10:25:24.132710   80575 exec_runner.go:144] found /home/sibichandran/.minikube/cert.pem, removing ...
I0128 10:25:24.132712   80575 exec_runner.go:203] rm: /home/sibichandran/.minikube/cert.pem
I0128 10:25:24.132731   80575 exec_runner.go:151] cp: /home/sibichandran/.minikube/certs/cert.pem --> /home/sibichandran/.minikube/cert.pem (1139 bytes)
I0128 10:25:24.132754   80575 exec_runner.go:144] found /home/sibichandran/.minikube/key.pem, removing ...
I0128 10:25:24.132756   80575 exec_runner.go:203] rm: /home/sibichandran/.minikube/key.pem
I0128 10:25:24.132770   80575 exec_runner.go:151] cp: /home/sibichandran/.minikube/certs/key.pem --> /home/sibichandran/.minikube/key.pem (1675 bytes)
I0128 10:25:24.132792   80575 provision.go:117] generating server cert: /home/sibichandran/.minikube/machines/server.pem ca-key=/home/sibichandran/.minikube/certs/ca.pem private-key=/home/sibichandran/.minikube/certs/ca-key.pem org=sibichandran.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0128 10:25:24.199680   80575 provision.go:177] copyRemoteCerts
I0128 10:25:24.199715   80575 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0128 10:25:24.199735   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:24.213282   80575 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/sibichandran/.minikube/machines/minikube/id_rsa Username:docker}
I0128 10:25:24.303440   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0128 10:25:24.320914   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1094 bytes)
I0128 10:25:24.338666   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I0128 10:25:24.357442   80575 provision.go:87] duration metric: took 237.005632ms to configureAuth
I0128 10:25:24.357456   80575 ubuntu.go:193] setting minikube options for container-runtime
I0128 10:25:24.357552   80575 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0128 10:25:24.357576   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:24.370389   80575 main.go:141] libmachine: Using SSH client type: native
I0128 10:25:24.370533   80575 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0128 10:25:24.370540   80575 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0128 10:25:24.490750   80575 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0128 10:25:24.490760   80575 ubuntu.go:71] root file system type: overlay
I0128 10:25:24.490881   80575 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0128 10:25:24.490914   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:24.506545   80575 main.go:141] libmachine: Using SSH client type: native
I0128 10:25:24.506819   80575 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0128 10:25:24.506921   80575 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0128 10:25:24.639993   80575 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0128 10:25:24.640052   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:24.651748   80575 main.go:141] libmachine: Using SSH client type: native
I0128 10:25:24.651910   80575 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32787 <nil> <nil>}
I0128 10:25:24.651923   80575 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0128 10:25:24.784222   80575 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0128 10:25:24.784234   80575 machine.go:96] duration metric: took 4.10118347s to provisionDockerMachine
I0128 10:25:24.784241   80575 start.go:293] postStartSetup for "minikube" (driver="docker")
I0128 10:25:24.784247   80575 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0128 10:25:24.784283   80575 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0128 10:25:24.784305   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:24.797137   80575 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/sibichandran/.minikube/machines/minikube/id_rsa Username:docker}
I0128 10:25:24.884565   80575 ssh_runner.go:195] Run: cat /etc/os-release
I0128 10:25:24.887203   80575 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0128 10:25:24.887215   80575 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0128 10:25:24.887219   80575 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0128 10:25:24.887222   80575 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0128 10:25:24.887230   80575 filesync.go:126] Scanning /home/sibichandran/.minikube/addons for local assets ...
I0128 10:25:24.887267   80575 filesync.go:126] Scanning /home/sibichandran/.minikube/files for local assets ...
I0128 10:25:24.887275   80575 start.go:296] duration metric: took 103.031258ms for postStartSetup
I0128 10:25:24.887301   80575 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0128 10:25:24.887319   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:24.904069   80575 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/sibichandran/.minikube/machines/minikube/id_rsa Username:docker}
I0128 10:25:24.991540   80575 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0128 10:25:24.996246   80575 fix.go:56] duration metric: took 4.851532724s for fixHost
I0128 10:25:24.996257   80575 start.go:83] releasing machines lock for "minikube", held for 4.851555423s
I0128 10:25:24.996293   80575 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0128 10:25:25.008928   80575 ssh_runner.go:195] Run: cat /version.json
I0128 10:25:25.008950   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:25.009064   80575 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0128 10:25:25.009111   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:25.023695   80575 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/sibichandran/.minikube/machines/minikube/id_rsa Username:docker}
I0128 10:25:25.026367   80575 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/sibichandran/.minikube/machines/minikube/id_rsa Username:docker}
I0128 10:25:25.343798   80575 ssh_runner.go:195] Run: systemctl --version
I0128 10:25:25.347132   80575 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0128 10:25:25.351211   80575 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0128 10:25:25.367138   80575 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0128 10:25:25.367189   80575 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0128 10:25:25.375737   80575 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0128 10:25:25.375751   80575 start.go:495] detecting cgroup driver to use...
I0128 10:25:25.375777   80575 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0128 10:25:25.375984   80575 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0128 10:25:25.388888   80575 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0128 10:25:25.398324   80575 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0128 10:25:25.409362   80575 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0128 10:25:25.409395   80575 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0128 10:25:25.418187   80575 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0128 10:25:25.426475   80575 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0128 10:25:25.434125   80575 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0128 10:25:25.441192   80575 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0128 10:25:25.449036   80575 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0128 10:25:25.457726   80575 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0128 10:25:25.465582   80575 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0128 10:25:25.473182   80575 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0128 10:25:25.479136   80575 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0128 10:25:25.484696   80575 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 10:25:25.537958   80575 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0128 10:25:25.733371   80575 start.go:495] detecting cgroup driver to use...
I0128 10:25:25.733395   80575 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0128 10:25:25.733416   80575 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0128 10:25:25.745923   80575 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0128 10:25:25.745956   80575 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0128 10:25:25.754699   80575 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0128 10:25:25.772750   80575 ssh_runner.go:195] Run: which cri-dockerd
I0128 10:25:25.777585   80575 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0128 10:25:25.784642   80575 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0128 10:25:25.797211   80575 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0128 10:25:25.877891   80575 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0128 10:25:25.932417   80575 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0128 10:25:25.932481   80575 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0128 10:25:25.942286   80575 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 10:25:26.032525   80575 ssh_runner.go:195] Run: sudo systemctl restart docker
I0128 10:25:29.971440   80575 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.938894992s)
I0128 10:25:29.971483   80575 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0128 10:25:29.978876   80575 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0128 10:25:29.987819   80575 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0128 10:25:29.996861   80575 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0128 10:25:30.082412   80575 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0128 10:25:30.153103   80575 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 10:25:30.224719   80575 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0128 10:25:30.235015   80575 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0128 10:25:30.243558   80575 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 10:25:30.315433   80575 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0128 10:25:30.375963   80575 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0128 10:25:30.376006   80575 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0128 10:25:30.379789   80575 start.go:563] Will wait 60s for crictl version
I0128 10:25:30.379827   80575 ssh_runner.go:195] Run: which crictl
I0128 10:25:30.383577   80575 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0128 10:25:30.416010   80575 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0128 10:25:30.416055   80575 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0128 10:25:30.436523   80575 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0128 10:25:30.459373   80575 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0128 10:25:30.459538   80575 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0128 10:25:30.469625   80575 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0128 10:25:30.473159   80575 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0128 10:25:30.480916   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0128 10:25:30.493286   80575 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sibichandran:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0128 10:25:30.493348   80575 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0128 10:25:30.493373   80575 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0128 10:25:30.509203   80575 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0128 10:25:30.509220   80575 docker.go:619] Images already preloaded, skipping extraction
I0128 10:25:30.509252   80575 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0128 10:25:30.525158   80575 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0128 10:25:30.525168   80575 cache_images.go:84] Images are preloaded, skipping loading
I0128 10:25:30.525172   80575 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0128 10:25:30.525225   80575 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0128 10:25:30.525256   80575 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0128 10:25:30.584107   80575 cni.go:84] Creating CNI manager for ""
I0128 10:25:30.584121   80575 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0128 10:25:30.584151   80575 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0128 10:25:30.584169   80575 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0128 10:25:30.584343   80575 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0128 10:25:30.584376   80575 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0128 10:25:30.592846   80575 binaries.go:44] Found k8s binaries, skipping transfer
I0128 10:25:30.592875   80575 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0128 10:25:30.600752   80575 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0128 10:25:30.614330   80575 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0128 10:25:30.628434   80575 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0128 10:25:30.642540   80575 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0128 10:25:30.645742   80575 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0128 10:25:30.653955   80575 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 10:25:30.696597   80575 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0128 10:25:30.705702   80575 certs.go:68] Setting up /home/sibichandran/.minikube/profiles/minikube for IP: 192.168.49.2
I0128 10:25:30.705711   80575 certs.go:194] generating shared ca certs ...
I0128 10:25:30.705722   80575 certs.go:226] acquiring lock for ca certs: {Name:mk43b2553a6ccdfac221b41db68bea64c3034582 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 10:25:30.705835   80575 certs.go:235] skipping valid "minikubeCA" ca cert: /home/sibichandran/.minikube/ca.key
I0128 10:25:30.705852   80575 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/sibichandran/.minikube/proxy-client-ca.key
I0128 10:25:30.705856   80575 certs.go:256] generating profile certs ...
I0128 10:25:30.705898   80575 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/sibichandran/.minikube/profiles/minikube/client.key
I0128 10:25:30.705917   80575 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/sibichandran/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0128 10:25:30.705931   80575 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/sibichandran/.minikube/profiles/minikube/proxy-client.key
I0128 10:25:30.705976   80575 certs.go:484] found cert: /home/sibichandran/.minikube/certs/ca-key.pem (1675 bytes)
I0128 10:25:30.705987   80575 certs.go:484] found cert: /home/sibichandran/.minikube/certs/ca.pem (1094 bytes)
I0128 10:25:30.705995   80575 certs.go:484] found cert: /home/sibichandran/.minikube/certs/cert.pem (1139 bytes)
I0128 10:25:30.706004   80575 certs.go:484] found cert: /home/sibichandran/.minikube/certs/key.pem (1675 bytes)
I0128 10:25:30.706444   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0128 10:25:30.722269   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0128 10:25:30.738340   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0128 10:25:30.755381   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0128 10:25:30.771061   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0128 10:25:30.811830   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0128 10:25:30.836267   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0128 10:25:30.931532   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0128 10:25:31.010781   80575 ssh_runner.go:362] scp /home/sibichandran/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0128 10:25:31.052101   80575 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0128 10:25:31.125005   80575 ssh_runner.go:195] Run: openssl version
I0128 10:25:31.133677   80575 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0128 10:25:31.146574   80575 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0128 10:25:31.150989   80575 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jan 28 09:51 /usr/share/ca-certificates/minikubeCA.pem
I0128 10:25:31.151022   80575 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0128 10:25:31.158117   80575 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0128 10:25:31.165261   80575 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0128 10:25:31.168422   80575 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0128 10:25:31.173608   80575 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0128 10:25:31.178615   80575 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0128 10:25:31.183398   80575 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0128 10:25:31.208568   80575 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0128 10:25:31.212871   80575 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0128 10:25:31.218046   80575 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/sibichandran:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0128 10:25:31.218131   80575 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0128 10:25:31.232734   80575 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0128 10:25:31.241088   80575 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0128 10:25:31.241104   80575 kubeadm.go:593] restartPrimaryControlPlane start ...
I0128 10:25:31.241568   80575 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0128 10:25:31.251132   80575 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0128 10:25:31.251231   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0128 10:25:31.268011   80575 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/sibichandran/.kube/config
I0128 10:25:31.268075   80575 kubeconfig.go:62] /home/sibichandran/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0128 10:25:31.268254   80575 lock.go:35] WriteFile acquiring /home/sibichandran/.kube/config: {Name:mk95f8fdcc8de09ecff80f2b6d06b39e1810b3ae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 10:25:31.270085   80575 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0128 10:25:31.307958   80575 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0128 10:25:31.307976   80575 kubeadm.go:597] duration metric: took 66.869407ms to restartPrimaryControlPlane
I0128 10:25:31.307981   80575 kubeadm.go:394] duration metric: took 89.943903ms to StartCluster
I0128 10:25:31.307998   80575 settings.go:142] acquiring lock: {Name:mk30bbd012b330d14a1c92d23d08a7829f860008 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 10:25:31.308062   80575 settings.go:150] Updating kubeconfig:  /home/sibichandran/.kube/config
I0128 10:25:31.308573   80575 lock.go:35] WriteFile acquiring /home/sibichandran/.kube/config: {Name:mk95f8fdcc8de09ecff80f2b6d06b39e1810b3ae Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0128 10:25:31.309702   80575 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0128 10:25:31.309874   80575 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0128 10:25:31.309935   80575 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0128 10:25:31.309947   80575 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0128 10:25:31.309952   80575 addons.go:247] addon storage-provisioner should already be in state true
I0128 10:25:31.309974   80575 host.go:66] Checking if "minikube" exists ...
I0128 10:25:31.309977   80575 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0128 10:25:31.310010   80575 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0128 10:25:31.310023   80575 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0128 10:25:31.310287   80575 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 10:25:31.310293   80575 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 10:25:31.315541   80575 out.go:177] 🔎  Verifying Kubernetes components...
I0128 10:25:31.325578   80575 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0128 10:25:31.335027   80575 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0128 10:25:31.335036   80575 addons.go:247] addon default-storageclass should already be in state true
I0128 10:25:31.335052   80575 host.go:66] Checking if "minikube" exists ...
I0128 10:25:31.335356   80575 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0128 10:25:31.335466   80575 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0128 10:25:31.342662   80575 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0128 10:25:31.342674   80575 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0128 10:25:31.342728   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:31.354673   80575 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0128 10:25:31.354686   80575 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0128 10:25:31.354737   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0128 10:25:31.365260   80575 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/sibichandran/.minikube/machines/minikube/id_rsa Username:docker}
I0128 10:25:31.375618   80575 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32787 SSHKeyPath:/home/sibichandran/.minikube/machines/minikube/id_rsa Username:docker}
I0128 10:25:31.411959   80575 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0128 10:25:31.429966   80575 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0128 10:25:31.449811   80575 api_server.go:52] waiting for apiserver process to appear ...
I0128 10:25:31.449871   80575 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 10:25:31.528695   80575 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0128 10:25:31.529640   80575 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0128 10:25:31.728766   80575 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0128 10:25:31.728809   80575 retry.go:31] will retry after 206.692157ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0128 10:25:31.728965   80575 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0128 10:25:31.728978   80575 retry.go:31] will retry after 333.154636ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0128 10:25:31.936374   80575 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0128 10:25:31.950561   80575 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0128 10:25:32.062526   80575 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0128 10:25:34.579219   80575 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.642808814s)
I0128 10:25:34.579311   80575 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.628730145s)
I0128 10:25:34.579331   80575 api_server.go:72] duration metric: took 3.269584973s to wait for apiserver process to appear ...
I0128 10:25:34.579334   80575 api_server.go:88] waiting for apiserver healthz status ...
I0128 10:25:34.579346   80575 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32784/healthz ...
I0128 10:25:34.579487   80575 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.516948609s)
I0128 10:25:34.583175   80575 api_server.go:279] https://127.0.0.1:32784/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0128 10:25:34.583186   80575 api_server.go:103] status: https://127.0.0.1:32784/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0128 10:25:34.593304   80575 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0128 10:25:34.594562   80575 addons.go:514] duration metric: took 3.284696586s for enable addons: enabled=[storage-provisioner default-storageclass]
I0128 10:25:35.080079   80575 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32784/healthz ...
I0128 10:25:35.083289   80575 api_server.go:279] https://127.0.0.1:32784/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0128 10:25:35.083312   80575 api_server.go:103] status: https://127.0.0.1:32784/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0128 10:25:35.580328   80575 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32784/healthz ...
I0128 10:25:35.586490   80575 api_server.go:279] https://127.0.0.1:32784/healthz returned 200:
ok
I0128 10:25:35.588604   80575 api_server.go:141] control plane version: v1.32.0
I0128 10:25:35.588620   80575 api_server.go:131] duration metric: took 1.009282658s to wait for apiserver health ...
I0128 10:25:35.588626   80575 system_pods.go:43] waiting for kube-system pods to appear ...
I0128 10:25:35.596724   80575 system_pods.go:59] 7 kube-system pods found
I0128 10:25:35.596778   80575 system_pods.go:61] "coredns-668d6bf9bc-bwnwc" [0524d496-41fb-40e1-94fe-624632a8d98d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0128 10:25:35.596782   80575 system_pods.go:61] "etcd-minikube" [8d1e65a8-889a-43f8-95e6-5a1e62b15cd7] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0128 10:25:35.596785   80575 system_pods.go:61] "kube-apiserver-minikube" [15d3d1a9-3ca6-400c-a6a9-e648d216220c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0128 10:25:35.596805   80575 system_pods.go:61] "kube-controller-manager-minikube" [96b00262-ae0c-43e9-bc27-e504a9e6c718] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0128 10:25:35.596807   80575 system_pods.go:61] "kube-proxy-h4fcq" [0f15b42d-5825-4613-a42d-bb48ad1561bc] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0128 10:25:35.596809   80575 system_pods.go:61] "kube-scheduler-minikube" [c6167383-c9f0-45f7-85ff-aafda4b33303] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0128 10:25:35.596811   80575 system_pods.go:61] "storage-provisioner" [e7635085-1fcb-4d8f-b571-7e15ec479127] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0128 10:25:35.596815   80575 system_pods.go:74] duration metric: took 8.185619ms to wait for pod list to return data ...
I0128 10:25:35.596822   80575 kubeadm.go:582] duration metric: took 4.287081432s to wait for: map[apiserver:true system_pods:true]
I0128 10:25:35.596830   80575 node_conditions.go:102] verifying NodePressure condition ...
I0128 10:25:35.611957   80575 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0128 10:25:35.611981   80575 node_conditions.go:123] node cpu capacity is 12
I0128 10:25:35.611994   80575 node_conditions.go:105] duration metric: took 15.160595ms to run NodePressure ...
I0128 10:25:35.612009   80575 start.go:241] waiting for startup goroutines ...
I0128 10:25:35.612016   80575 start.go:246] waiting for cluster config update ...
I0128 10:25:35.612025   80575 start.go:255] writing updated cluster config ...
I0128 10:25:35.612992   80575 ssh_runner.go:195] Run: rm -f paused
I0128 10:25:35.871902   80575 start.go:600] kubectl: 1.31.5, cluster: 1.32.0 (minor skew: 1)
I0128 10:25:35.875941   80575 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jan 28 10:25:26 minikube dockerd[820]: time="2025-01-28T10:25:26.039538650Z" level=info msg="Processing signal 'terminated'"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.341391658Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.648739593Z" level=info msg="Loading containers: done."
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.703347393Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.703377371Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.703381391Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.703384221Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.703397366Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.703440258Z" level=info msg="Daemon has completed initialization"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.914954165Z" level=info msg="API listen on /var/run/docker.sock"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.914989457Z" level=info msg="API listen on [::]:2376"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.915970060Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.916442441Z" level=info msg="Daemon shutdown complete"
Jan 28 10:25:27 minikube dockerd[820]: time="2025-01-28T10:25:27.916484102Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Jan 28 10:25:27 minikube systemd[1]: docker.service: Deactivated successfully.
Jan 28 10:25:27 minikube systemd[1]: Stopped Docker Application Container Engine.
Jan 28 10:25:27 minikube systemd[1]: Starting Docker Application Container Engine...
Jan 28 10:25:27 minikube dockerd[1116]: time="2025-01-28T10:25:27.946560175Z" level=info msg="Starting up"
Jan 28 10:25:27 minikube dockerd[1116]: time="2025-01-28T10:25:27.947987964Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jan 28 10:25:27 minikube dockerd[1116]: time="2025-01-28T10:25:27.969220452Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jan 28 10:25:27 minikube dockerd[1116]: time="2025-01-28T10:25:27.982225764Z" level=info msg="Loading containers: start."
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.476951602Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.728860573Z" level=info msg="Loading containers: done."
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.742653427Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.742702680Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.742714471Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.742720662Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.742740518Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.742795142Z" level=info msg="Daemon has completed initialization"
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.969951901Z" level=info msg="API listen on /var/run/docker.sock"
Jan 28 10:25:29 minikube dockerd[1116]: time="2025-01-28T10:25:29.970019487Z" level=info msg="API listen on [::]:2376"
Jan 28 10:25:29 minikube systemd[1]: Started Docker Application Container Engine.
Jan 28 10:25:30 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Start docker client with request timeout 0s"
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Loaded network plugin cni"
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Docker cri networking managed by network plugin cni"
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Setting cgroupDriver cgroupfs"
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 28 10:25:30 minikube cri-dockerd[1415]: time="2025-01-28T10:25:30Z" level=info msg="Start cri-dockerd grpc backend"
Jan 28 10:25:30 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 28 10:25:31 minikube cri-dockerd[1415]: time="2025-01-28T10:25:31Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-bwnwc_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"60a1b0031863a7c9fa185f50003fb6e6f1952a11deaae4e37a3381b2f0c269b2\""
Jan 28 10:25:31 minikube cri-dockerd[1415]: time="2025-01-28T10:25:31Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-bwnwc_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"912fbf6fd0a492fd977ec059ff19247a7d806ffe92dfdb679f645ae26bb76fc6\""
Jan 28 10:25:31 minikube cri-dockerd[1415]: time="2025-01-28T10:25:31Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"d133013aa6a20d32d6809724a68c1b82e2b6f368e16cbc75fd396ed694163674\". Proceed without further sandbox information."
Jan 28 10:25:31 minikube cri-dockerd[1415]: time="2025-01-28T10:25:31Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"0069019ee1c59ef4ee8efce59b794bee5a84d05424eb43b5a6231a38e7f0abad\". Proceed without further sandbox information."
Jan 28 10:25:31 minikube cri-dockerd[1415]: time="2025-01-28T10:25:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/04d560a8023244cc8c395c81367886af378b964b45a014145fa99a80783f69af/resolv.conf as [nameserver 192.168.0.128 options ndots:0]"
Jan 28 10:25:31 minikube cri-dockerd[1415]: time="2025-01-28T10:25:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c695320dc296f45e4af5b135c9ed9def775e2af631a66b51e7f57c8562b20b68/resolv.conf as [nameserver 192.168.0.128 options ndots:0]"
Jan 28 10:25:31 minikube cri-dockerd[1415]: time="2025-01-28T10:25:31Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3d4cdebead8772624545a8d4d704c8556a761a0fb33a3ef88490332bc62811b4/resolv.conf as [nameserver 192.168.0.128 options ndots:0]"
Jan 28 10:25:32 minikube cri-dockerd[1415]: time="2025-01-28T10:25:32Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-bwnwc_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"60a1b0031863a7c9fa185f50003fb6e6f1952a11deaae4e37a3381b2f0c269b2\""
Jan 28 10:25:32 minikube cri-dockerd[1415]: time="2025-01-28T10:25:32Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a581e6fc1aca86b63a02c10afa6e05eddc1a39421e1a866571ef6d1a83869995/resolv.conf as [nameserver 192.168.0.128 options ndots:0]"
Jan 28 10:25:34 minikube cri-dockerd[1415]: time="2025-01-28T10:25:34Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 28 10:25:35 minikube cri-dockerd[1415]: time="2025-01-28T10:25:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/67f7661d95de83920765463a3d530bdaf67d8cb926a7f1846457cd5e57abc389/resolv.conf as [nameserver 192.168.0.128 options ndots:0]"
Jan 28 10:25:35 minikube cri-dockerd[1415]: time="2025-01-28T10:25:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5c08a1601385294cedb946e31e1676233973059ff43d9026ba9c2df938f130ae/resolv.conf as [nameserver 192.168.0.128 options ndots:0]"
Jan 28 10:25:35 minikube cri-dockerd[1415]: time="2025-01-28T10:25:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/84aa84fb831b8eceaadb3391dd3d7fe2217e2d2e1a884b5b4e10037543eaa38b/resolv.conf as [nameserver 192.168.0.128 options ndots:0]"
Jan 28 10:26:05 minikube dockerd[1116]: time="2025-01-28T10:26:05.758096408Z" level=info msg="ignoring event" container=204c8c2002e47e21032e9b86a3fca3355783c393efa1e774b2b9ca7c8b3c7957 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 28 10:55:46 minikube cri-dockerd[1415]: time="2025-01-28T10:55:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/943fa2c2d9126f74f3b4cfe6d2d1e220f49eae6c7fd763c61251801925a738cf/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 28 11:04:59 minikube dockerd[1116]: time="2025-01-28T11:04:59.532555137Z" level=error msg="Handler for POST /v1.45/build returned error: unexpected EOF"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
a27d223a200b6       6e38f40d628db       42 minutes ago      Running             storage-provisioner       5                   67f7661d95de8       storage-provisioner
13a5b23df9da9       c69fa2e9cbf5f       43 minutes ago      Running             coredns                   2                   84aa84fb831b8       coredns-668d6bf9bc-bwnwc
bb44a43c3ea42       040f9f8aac8cd       43 minutes ago      Running             kube-proxy                2                   5c08a16013852       kube-proxy-h4fcq
204c8c2002e47       6e38f40d628db       43 minutes ago      Exited              storage-provisioner       4                   67f7661d95de8       storage-provisioner
97b4624d49060       8cab3d2a8bd0f       43 minutes ago      Running             kube-controller-manager   2                   a581e6fc1aca8       kube-controller-manager-minikube
f4fdb70e42908       a389e107f4ff1       43 minutes ago      Running             kube-scheduler            2                   3d4cdebead877       kube-scheduler-minikube
b9c829b45b3cc       a9e7e6b294baf       43 minutes ago      Running             etcd                      2                   c695320dc296f       etcd-minikube
c64dd835cbe06       c2e17b8d0f4a3       43 minutes ago      Running             kube-apiserver            2                   04d560a802324       kube-apiserver-minikube
ff219c24f01fe       c69fa2e9cbf5f       About an hour ago   Exited              coredns                   1                   60a1b0031863a       coredns-668d6bf9bc-bwnwc
64d83f9cc6698       a9e7e6b294baf       About an hour ago   Exited              etcd                      1                   4897b14d9134f       etcd-minikube
2c44eda3b30d0       8cab3d2a8bd0f       About an hour ago   Exited              kube-controller-manager   1                   c976d938324b9       kube-controller-manager-minikube
17fe15c2a585b       c2e17b8d0f4a3       About an hour ago   Exited              kube-apiserver            1                   8a9c011aa1053       kube-apiserver-minikube
43f924dc05319       040f9f8aac8cd       About an hour ago   Exited              kube-proxy                1                   72726b7b06ec3       kube-proxy-h4fcq
ec9dbc749a2d4       a389e107f4ff1       About an hour ago   Exited              kube-scheduler            1                   da5d435382100       kube-scheduler-minikube


==> coredns [13a5b23df9da] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:34520 - 57559 "HINFO IN 4229801227849081400.4845981063200136216. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.039332159s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1608741202]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (28-Jan-2025 10:25:35.854) (total time: 30001ms):
Trace[1608741202]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (10:26:05.855)
Trace[1608741202]: [30.001048699s] [30.001048699s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1960553169]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (28-Jan-2025 10:25:35.854) (total time: 30000ms):
Trace[1960553169]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (10:26:05.855)
Trace[1960553169]: [30.000828036s] [30.000828036s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1105541147]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (28-Jan-2025 10:25:35.854) (total time: 30000ms):
Trace[1105541147]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (10:26:05.855)
Trace[1105541147]: [30.000938451s] [30.000938451s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> coredns [ff219c24f01f] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:44755 - 48515 "HINFO IN 1537153439303956371.3075294850483070174. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.090145524s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_01_28T09_52_05_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 28 Jan 2025 09:52:02 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 28 Jan 2025 11:09:14 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 28 Jan 2025 11:05:32 +0000   Tue, 28 Jan 2025 09:52:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 28 Jan 2025 11:05:32 +0000   Tue, 28 Jan 2025 09:52:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 28 Jan 2025 11:05:32 +0000   Tue, 28 Jan 2025 09:52:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 28 Jan 2025 11:05:32 +0000   Tue, 28 Jan 2025 09:52:02 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7953584Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7953584Ki
  pods:               110
System Info:
  Machine ID:                 221fdafce7c14656aecf211d8bb4c68a
  System UUID:                221fdafce7c14656aecf211d8bb4c68a
  Boot ID:                    3f6f5d3d-6709-48ac-9fc9-73156f431575
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     nodejs-app-bbdff55d-mbc8k           0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 coredns-668d6bf9bc-bwnwc            100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     77m
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         77m
  kube-system                 kube-apiserver-minikube             250m (2%)     0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 kube-proxy-h4fcq                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         77m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         77m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           77m                kube-proxy       
  Normal   Starting                           43m                kube-proxy       
  Normal   Starting                           60m                kube-proxy       
  Warning  CgroupV1                           77m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   Starting                           77m                kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced            77m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            77m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              77m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               77m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  77m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   RegisteredNode                     77m                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  ContainerGCFailed                  61m                kubelet          rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
  Normal   RegisteredNode                     60m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeHasSufficientMemory            43m (x8 over 43m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                           43m                kubelet          Starting kubelet.
  Warning  CgroupV1                           43m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Warning  PossibleMemoryBackedVolumesOnDisk  43m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeHasNoDiskPressure              43m (x8 over 43m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               43m (x7 over 43m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            43m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     43m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Jan28 05:59] PCI: Fatal: No config space access function found
[  +0.008631] PCI: System does not support PCI
[  +0.099223] kvm: already loaded the other module
[  +0.609557] FS-Cache: Duplicate cookie detected
[  +0.000397] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000338] FS-Cache: O-cookie d=000000003b76d61c{9P.session} n=00000000d92b6c2a
[  +0.000366] FS-Cache: O-key=[10] '34323934393337333639'
[  +0.000241] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000285] FS-Cache: N-cookie d=000000003b76d61c{9P.session} n=00000000f636b37d
[  +0.000333] FS-Cache: N-key=[10] '34323934393337333639'
[  +0.002087] FS-Cache: Duplicate cookie detected
[  +0.000483] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000319] FS-Cache: O-cookie d=000000003b76d61c{9P.session} n=00000000d92b6c2a
[  +0.000417] FS-Cache: O-key=[10] '34323934393337333639'
[  +0.000214] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000237] FS-Cache: N-cookie d=000000003b76d61c{9P.session} n=00000000c110a996
[  +0.000255] FS-Cache: N-key=[10] '34323934393337333639'
[  +0.033932] FS-Cache: Duplicate cookie detected
[  +0.000349] FS-Cache: O-cookie c=00000008 [p=00000002 fl=222 nc=0 na=1]
[  +0.000288] FS-Cache: O-cookie d=000000003b76d61c{9P.session} n=00000000fcffb7e8
[  +0.000354] FS-Cache: O-key=[10] '34323934393337333733'
[  +0.000294] FS-Cache: N-cookie c=00000009 [p=00000002 fl=2 nc=0 na=1]
[  +0.000380] FS-Cache: N-cookie d=000000003b76d61c{9P.session} n=0000000062a240a6
[  +0.000370] FS-Cache: N-key=[10] '34323934393337333733'
[  +0.007487] FS-Cache: Duplicate cookie detected
[  +0.000328] FS-Cache: O-cookie c=0000000a [p=00000002 fl=222 nc=0 na=1]
[  +0.000223] FS-Cache: O-cookie d=000000003b76d61c{9P.session} n=00000000cc49a1d1
[  +0.000289] FS-Cache: O-key=[10] '34323934393337333734'
[  +0.000174] FS-Cache: N-cookie c=0000000b [p=00000002 fl=2 nc=0 na=1]
[  +0.000207] FS-Cache: N-cookie d=000000003b76d61c{9P.session} n=00000000bd24899a
[  +0.000237] FS-Cache: N-key=[10] '34323934393337333734'
[  +0.534348] WSL (2) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.067226] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.003818] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000415] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000379] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000382] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.439752] Failed to connect to bus: No such file or directory
[  +0.351773] systemd-journald[51]: File /var/log/journal/3dfdb378df7b4f7488828ce69e5fdffb/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +0.798097] systemd-journald[51]: Failed to read journal file /var/log/journal/3dfdb378df7b4f7488828ce69e5fdffb/user-1000.journal for rotation, trying to move it out of the way: Device or resource busy
[  +0.977143] TCP: eth0: Driver has suspect GRO implementation, TCP performance may be compromised.
[Jan28 06:45] tmpfs: Unknown parameter 'noswap'
[  +5.780117] tmpfs: Unknown parameter 'noswap'
[Jan28 08:38] tmpfs: Unknown parameter 'noswap'
[  +5.385652] tmpfs: Unknown parameter 'noswap'
[Jan28 09:46] tmpfs: Unknown parameter 'noswap'
[  +5.018199] tmpfs: Unknown parameter 'noswap'
[Jan28 10:19] tmpfs: Unknown parameter 'noswap'


==> etcd [64d83f9cc669] <==
{"level":"info","ts":"2025-01-28T10:08:15.530892Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"6.701465ms"}
{"level":"info","ts":"2025-01-28T10:08:15.614496Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2025-01-28T10:08:15.633123Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":1331}
{"level":"info","ts":"2025-01-28T10:08:15.633282Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-01-28T10:08:15.633332Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-01-28T10:08:15.633355Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 1331, applied: 0, lastindex: 1331, lastterm: 2]"}
{"level":"warn","ts":"2025-01-28T10:08:15.638866Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-01-28T10:08:15.641617Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":840}
{"level":"info","ts":"2025-01-28T10:08:15.709924Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1125}
{"level":"info","ts":"2025-01-28T10:08:15.712280Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-01-28T10:08:15.715623Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-01-28T10:08:15.716037Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-01-28T10:08:15.716077Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-01-28T10:08:15.716618Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-28T10:08:15.718147Z","caller":"etcdserver/server.go:773","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-01-28T10:08:15.718774Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-01-28T10:08:15.718867Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-01-28T10:08:15.718485Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-01-28T10:08:15.718982Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-01-28T10:08:15.719010Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-01-28T10:08:15.719027Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-01-28T10:08:15.719050Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-01-28T10:08:15.719165Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-01-28T10:08:15.719663Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-01-28T10:08:15.719697Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-01-28T10:08:15.719697Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-01-28T10:08:15.719720Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-01-28T10:08:17.437120Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-01-28T10:08:17.437154Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-01-28T10:08:17.437192Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-01-28T10:08:17.437201Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-01-28T10:08:17.437209Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-01-28T10:08:17.437220Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-01-28T10:08:17.437224Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-01-28T10:08:17.454831Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-01-28T10:08:17.454836Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-01-28T10:08:17.455024Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-01-28T10:08:17.455268Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-28T10:08:17.455278Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-28T10:08:17.455598Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-01-28T10:08:17.455600Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-01-28T10:08:17.460064Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-01-28T10:08:17.460104Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-01-28T10:18:17.472218Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1483}
{"level":"info","ts":"2025-01-28T10:18:17.475398Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1483,"took":"3.063847ms","hash":2060453223,"current-db-size-bytes":1925120,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1925120,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-01-28T10:18:17.475479Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2060453223,"revision":1483,"compact-revision":840}
{"level":"info","ts":"2025-01-28T10:23:17.482430Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1722}
{"level":"info","ts":"2025-01-28T10:23:17.485692Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1722,"took":"3.080934ms","hash":2701555244,"current-db-size-bytes":1925120,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1413120,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-01-28T10:23:17.485758Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2701555244,"revision":1722,"compact-revision":1483}
{"level":"info","ts":"2025-01-28T10:25:13.312445Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-01-28T10:25:13.312512Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-01-28T10:25:13.312590Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-01-28T10:25:13.312816Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
2025/01/28 10:25:13 WARNING: [core] [Server #7] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-01-28T10:25:13.407297Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-01-28T10:25:13.407358Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-01-28T10:25:13.407415Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-01-28T10:25:13.414467Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-01-28T10:25:13.414582Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-01-28T10:25:13.414668Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [b9c829b45b3c] <==
{"level":"info","ts":"2025-01-28T10:25:31.939271Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2025-01-28T10:25:31.939279Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 3, commit: 2474, applied: 0, lastindex: 2474, lastterm: 3]"}
{"level":"warn","ts":"2025-01-28T10:25:31.942086Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-01-28T10:25:31.943490Z","caller":"mvcc/kvstore.go:346","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1722}
{"level":"info","ts":"2025-01-28T10:25:31.944983Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":2057}
{"level":"info","ts":"2025-01-28T10:25:31.947060Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-01-28T10:25:31.949882Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-01-28T10:25:31.950223Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-01-28T10:25:31.950254Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-01-28T10:25:31.950627Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-28T10:25:31.950876Z","caller":"etcdserver/server.go:773","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-01-28T10:25:31.951134Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-01-28T10:25:31.951215Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-01-28T10:25:31.951291Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-01-28T10:25:31.951316Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-01-28T10:25:31.951958Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-01-28T10:25:31.952122Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-01-28T10:25:31.952138Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-01-28T10:25:31.952518Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-01-28T10:25:31.952968Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-01-28T10:25:31.953013Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-01-28T10:25:31.953057Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-01-28T10:25:31.953060Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-01-28T10:25:33.242163Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2025-01-28T10:25:33.242252Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2025-01-28T10:25:33.242276Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-01-28T10:25:33.242290Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2025-01-28T10:25:33.242295Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-01-28T10:25:33.242306Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2025-01-28T10:25:33.242312Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2025-01-28T10:25:33.247424Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-01-28T10:25:33.247909Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-01-28T10:25:33.247988Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-01-28T10:25:33.247997Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-01-28T10:25:33.247945Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-01-28T10:25:33.248507Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-28T10:25:33.248586Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-01-28T10:25:33.249071Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-01-28T10:25:33.249716Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-01-28T10:35:33.267349Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2364}
{"level":"info","ts":"2025-01-28T10:35:33.272205Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2364,"took":"4.303743ms","hash":2911311587,"current-db-size-bytes":2314240,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":2314240,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-01-28T10:35:33.272247Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2911311587,"revision":2364,"compact-revision":1722}
{"level":"info","ts":"2025-01-28T10:40:33.274081Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2604}
{"level":"info","ts":"2025-01-28T10:40:33.276794Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2604,"took":"2.57936ms","hash":1406166681,"current-db-size-bytes":2314240,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1449984,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-01-28T10:40:33.276828Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1406166681,"revision":2604,"compact-revision":2364}
{"level":"info","ts":"2025-01-28T10:45:33.280421Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2844}
{"level":"info","ts":"2025-01-28T10:45:33.283775Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2844,"took":"3.214342ms","hash":1454559283,"current-db-size-bytes":2314240,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1482752,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-01-28T10:45:33.283867Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1454559283,"revision":2844,"compact-revision":2604}
{"level":"info","ts":"2025-01-28T10:50:33.291009Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3085}
{"level":"info","ts":"2025-01-28T10:50:33.294628Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3085,"took":"3.475585ms","hash":3095734420,"current-db-size-bytes":2314240,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1515520,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-01-28T10:50:33.294738Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3095734420,"revision":3085,"compact-revision":2844}
{"level":"info","ts":"2025-01-28T10:55:33.298595Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3325}
{"level":"info","ts":"2025-01-28T10:55:33.301321Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3325,"took":"2.334314ms","hash":857461656,"current-db-size-bytes":2314240,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1511424,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-01-28T10:55:33.301363Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":857461656,"revision":3325,"compact-revision":3085}
{"level":"info","ts":"2025-01-28T11:00:33.304406Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3564}
{"level":"info","ts":"2025-01-28T11:00:33.306546Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3564,"took":"1.970876ms","hash":1438981956,"current-db-size-bytes":2314240,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1576960,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-01-28T11:00:33.306577Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1438981956,"revision":3564,"compact-revision":3325}
{"level":"info","ts":"2025-01-28T11:05:33.309766Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3852}
{"level":"info","ts":"2025-01-28T11:05:33.312722Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":3852,"took":"2.730778ms","hash":3729081958,"current-db-size-bytes":2314240,"current-db-size":"2.3 MB","current-db-size-in-use-bytes":1732608,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-01-28T11:05:33.312806Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3729081958,"revision":3852,"compact-revision":3564}


==> kernel <==
 11:09:14 up  5:09,  0 users,  load average: 0.55, 0.27, 0.20
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [17fe15c2a585] <==
I0128 10:25:13.323532       1 controller.go:157] Shutting down quota evaluator
I0128 10:25:13.323558       1 controller.go:176] quota evaluator worker shutdown
I0128 10:25:13.323574       1 controller.go:176] quota evaluator worker shutdown
I0128 10:25:13.323587       1 controller.go:176] quota evaluator worker shutdown
I0128 10:25:13.323598       1 controller.go:176] quota evaluator worker shutdown
I0128 10:25:13.323606       1 controller.go:176] quota evaluator worker shutdown
W0128 10:25:14.314882       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.315615       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.315639       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.315913       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.315939       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.315995       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316039       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316059       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316090       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316086       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316112       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316130       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316133       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316145       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316167       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316212       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316369       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316383       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316397       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316408       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316421       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316443       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316456       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316474       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316485       1 logging.go:55] [core] [Channel #19 SubChannel #20]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316506       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316558       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.316602       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323197       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323239       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323259       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323243       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323279       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323295       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323315       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323324       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323337       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323356       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323368       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323374       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323376       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323391       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323404       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323409       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323426       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323438       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323443       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323460       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323472       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323476       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323493       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323493       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323502       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0128 10:25:14.323627       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [c64dd835cbe0] <==
W0128 10:25:33.682108       1 genericapiserver.go:767] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0128 10:25:34.012831       1 secure_serving.go:213] Serving securely on [::]:8443
I0128 10:25:34.012861       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0128 10:25:34.012849       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0128 10:25:34.012840       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0128 10:25:34.012855       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0128 10:25:34.013027       1 controller.go:78] Starting OpenAPI AggregationController
I0128 10:25:34.013385       1 aggregator.go:169] waiting for initial CRD sync...
I0128 10:25:34.013458       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0128 10:25:34.013476       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0128 10:25:34.013497       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0128 10:25:34.013506       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0128 10:25:34.031382       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0128 10:25:34.013030       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0128 10:25:34.057289       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0128 10:25:34.057309       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0128 10:25:34.057336       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0128 10:25:34.057395       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0128 10:25:34.057606       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0128 10:25:34.057618       1 local_available_controller.go:156] Starting LocalAvailability controller
I0128 10:25:34.057621       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0128 10:25:34.058100       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0128 10:25:34.058110       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0128 10:25:34.058168       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0128 10:25:34.058180       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0128 10:25:34.058182       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0128 10:25:34.058209       1 controller.go:119] Starting legacy_token_tracking_controller
I0128 10:25:34.058211       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0128 10:25:34.058313       1 controller.go:142] Starting OpenAPI controller
I0128 10:25:34.058326       1 controller.go:90] Starting OpenAPI V3 controller
I0128 10:25:34.058336       1 naming_controller.go:294] Starting NamingConditionController
I0128 10:25:34.058342       1 establishing_controller.go:81] Starting EstablishingController
I0128 10:25:34.058348       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0128 10:25:34.058355       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0128 10:25:34.058360       1 crd_finalizer.go:269] Starting CRDFinalizer
I0128 10:25:34.107596       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0128 10:25:34.107621       1 policy_source.go:240] refreshing policies
I0128 10:25:34.113920       1 shared_informer.go:320] Caches are synced for node_authorizer
I0128 10:25:34.114290       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0128 10:25:34.115492       1 aggregator.go:171] initial CRD sync complete...
I0128 10:25:34.115529       1 autoregister_controller.go:144] Starting autoregister controller
I0128 10:25:34.115537       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0128 10:25:34.115543       1 cache.go:39] Caches are synced for autoregister controller
I0128 10:25:34.135264       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0128 10:25:34.135299       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0128 10:25:34.207533       1 cache.go:39] Caches are synced for LocalAvailability controller
I0128 10:25:34.207905       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0128 10:25:34.207993       1 shared_informer.go:320] Caches are synced for configmaps
I0128 10:25:34.208051       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0128 10:25:34.208200       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0128 10:25:34.208305       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0128 10:25:34.214236       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0128 10:25:34.913661       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0128 10:25:34.913661       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0128 10:25:35.016542       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0128 10:25:37.423969       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0128 10:25:37.622876       1 controller.go:615] quota admission added evaluator for: endpoints
I0128 10:25:37.673053       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0128 10:25:37.773875       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0128 10:55:46.245036       1 alloc.go:330] "allocated clusterIPs" service="default/nodejs-app" clusterIPs={"IPv4":"10.97.15.215"}


==> kube-controller-manager [2c44eda3b30d] <==
I0128 10:08:21.277904       1 controllermanager.go:765] "Started controller" controller="ttl-after-finished-controller"
I0128 10:08:21.277924       1 ttlafterfinished_controller.go:112] "Starting TTL after finished controller" logger="ttl-after-finished-controller"
I0128 10:08:21.277935       1 shared_informer.go:313] Waiting for caches to sync for TTL after finished
I0128 10:08:21.279815       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0128 10:08:21.286967       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0128 10:08:21.287292       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0128 10:08:21.327378       1 shared_informer.go:320] Caches are synced for daemon sets
I0128 10:08:21.327512       1 shared_informer.go:320] Caches are synced for crt configmap
I0128 10:08:21.327769       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0128 10:08:21.327802       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0128 10:08:21.328624       1 shared_informer.go:320] Caches are synced for cronjob
I0128 10:08:21.328638       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0128 10:08:21.328664       1 shared_informer.go:320] Caches are synced for expand
I0128 10:08:21.328687       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0128 10:08:21.328726       1 shared_informer.go:320] Caches are synced for taint
I0128 10:08:21.328766       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0128 10:08:21.328823       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0128 10:08:21.328849       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0128 10:08:21.330981       1 shared_informer.go:320] Caches are synced for attach detach
I0128 10:08:21.342472       1 shared_informer.go:320] Caches are synced for namespace
I0128 10:08:21.343866       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0128 10:08:21.343899       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0128 10:08:21.343910       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0128 10:08:21.343924       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0128 10:08:21.345036       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0128 10:08:21.353358       1 shared_informer.go:320] Caches are synced for node
I0128 10:08:21.353399       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0128 10:08:21.353419       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0128 10:08:21.353422       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0128 10:08:21.353424       1 shared_informer.go:320] Caches are synced for cidrallocator
I0128 10:08:21.353468       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:08:21.354826       1 shared_informer.go:320] Caches are synced for persistent volume
I0128 10:08:21.364242       1 shared_informer.go:320] Caches are synced for resource quota
I0128 10:08:21.364394       1 shared_informer.go:320] Caches are synced for service account
I0128 10:08:21.366607       1 shared_informer.go:320] Caches are synced for job
I0128 10:08:21.367918       1 shared_informer.go:320] Caches are synced for endpoint
I0128 10:08:21.369184       1 shared_informer.go:320] Caches are synced for GC
I0128 10:08:21.373684       1 shared_informer.go:320] Caches are synced for garbage collector
I0128 10:08:21.373703       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0128 10:08:21.373707       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0128 10:08:21.374919       1 shared_informer.go:320] Caches are synced for deployment
I0128 10:08:21.377655       1 shared_informer.go:320] Caches are synced for disruption
I0128 10:08:21.377674       1 shared_informer.go:320] Caches are synced for TTL
I0128 10:08:21.377697       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0128 10:08:21.377706       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0128 10:08:21.377758       1 shared_informer.go:320] Caches are synced for ReplicationController
I0128 10:08:21.377937       1 shared_informer.go:320] Caches are synced for PVC protection
I0128 10:08:21.377954       1 shared_informer.go:320] Caches are synced for TTL after finished
I0128 10:08:21.378964       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0128 10:08:21.379045       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="31.552µs"
I0128 10:08:21.380251       1 shared_informer.go:320] Caches are synced for HPA
I0128 10:08:21.380358       1 shared_informer.go:320] Caches are synced for resource quota
I0128 10:08:21.381710       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0128 10:08:21.383120       1 shared_informer.go:320] Caches are synced for PV protection
I0128 10:08:21.385672       1 shared_informer.go:320] Caches are synced for ephemeral
I0128 10:08:21.386870       1 shared_informer.go:320] Caches are synced for stateful set
I0128 10:08:21.388003       1 shared_informer.go:320] Caches are synced for garbage collector
I0128 10:10:16.018520       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:15:22.256996       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:20:28.982001       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [97b4624d4906] <==
I0128 10:25:37.322084       1 shared_informer.go:320] Caches are synced for disruption
I0128 10:25:37.322089       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0128 10:25:37.322171       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0128 10:25:37.322183       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0128 10:25:37.322178       1 shared_informer.go:320] Caches are synced for ephemeral
I0128 10:25:37.324192       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0128 10:25:37.334594       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0128 10:25:37.335840       1 shared_informer.go:320] Caches are synced for namespace
I0128 10:25:37.337000       1 shared_informer.go:320] Caches are synced for endpoint
I0128 10:25:37.338494       1 shared_informer.go:320] Caches are synced for stateful set
I0128 10:25:37.339892       1 shared_informer.go:320] Caches are synced for daemon sets
I0128 10:25:37.343910       1 shared_informer.go:320] Caches are synced for cronjob
I0128 10:25:37.343969       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0128 10:25:37.344045       1 shared_informer.go:320] Caches are synced for PVC protection
I0128 10:25:37.349272       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0128 10:25:37.356988       1 shared_informer.go:320] Caches are synced for garbage collector
I0128 10:25:37.357011       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0128 10:25:37.357018       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0128 10:25:37.360242       1 shared_informer.go:320] Caches are synced for taint
I0128 10:25:37.360304       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0128 10:25:37.360480       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0128 10:25:37.360510       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0128 10:25:37.362661       1 shared_informer.go:320] Caches are synced for expand
I0128 10:25:37.365052       1 shared_informer.go:320] Caches are synced for GC
I0128 10:25:37.371450       1 shared_informer.go:320] Caches are synced for TTL after finished
I0128 10:25:37.371497       1 shared_informer.go:320] Caches are synced for crt configmap
I0128 10:25:37.371524       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0128 10:25:37.371534       1 shared_informer.go:320] Caches are synced for PV protection
I0128 10:25:37.371533       1 shared_informer.go:320] Caches are synced for HPA
I0128 10:25:37.371807       1 shared_informer.go:320] Caches are synced for service account
I0128 10:25:37.374922       1 shared_informer.go:320] Caches are synced for node
I0128 10:25:37.374963       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0128 10:25:37.374972       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0128 10:25:37.374974       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0128 10:25:37.374977       1 shared_informer.go:320] Caches are synced for cidrallocator
I0128 10:25:37.375005       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:25:37.376236       1 shared_informer.go:320] Caches are synced for resource quota
I0128 10:25:37.377334       1 shared_informer.go:320] Caches are synced for deployment
I0128 10:25:37.378533       1 shared_informer.go:320] Caches are synced for resource quota
I0128 10:25:37.389957       1 shared_informer.go:320] Caches are synced for garbage collector
I0128 10:25:37.410297       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0128 10:25:37.410429       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0128 10:25:37.411833       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0128 10:25:37.676920       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="305.362162ms"
I0128 10:25:37.677424       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="28.348µs"
I0128 10:26:10.748192       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="16.026499ms"
I0128 10:26:10.748359       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="28.379µs"
I0128 10:29:49.040123       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:34:56.075162       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:40:01.031765       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:45:07.287524       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:50:13.379968       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:55:20.150036       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 10:55:46.253286       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-bbdff55d" duration="21.211088ms"
I0128 10:55:46.277767       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-bbdff55d" duration="24.426086ms"
I0128 10:55:46.277825       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-bbdff55d" duration="19.652µs"
I0128 10:55:46.309986       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-bbdff55d" duration="87.309µs"
I0128 10:55:47.374331       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/nodejs-app-bbdff55d" duration="26.946µs"
I0128 11:00:27.496540       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0128 11:05:32.571638       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [43f924dc0531] <==
I0128 10:08:15.413347       1 server_linux.go:66] "Using iptables proxy"
I0128 10:08:18.222418       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0128 10:08:18.222473       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0128 10:08:18.444735       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0128 10:08:18.444803       1 server_linux.go:170] "Using iptables Proxier"
I0128 10:08:18.448196       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0128 10:08:18.507329       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0128 10:08:18.530278       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0128 10:08:18.530419       1 server.go:497] "Version info" version="v1.32.0"
I0128 10:08:18.530430       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0128 10:08:18.548299       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0128 10:08:18.566178       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0128 10:08:18.607230       1 config.go:105] "Starting endpoint slice config controller"
I0128 10:08:18.607259       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0128 10:08:18.607289       1 config.go:199] "Starting service config controller"
I0128 10:08:18.607294       1 shared_informer.go:313] Waiting for caches to sync for service config
I0128 10:08:18.607305       1 config.go:329] "Starting node config controller"
I0128 10:08:18.607309       1 shared_informer.go:313] Waiting for caches to sync for node config
I0128 10:08:18.707858       1 shared_informer.go:320] Caches are synced for node config
I0128 10:08:18.707864       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0128 10:08:18.707875       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [bb44a43c3ea4] <==
I0128 10:25:35.810476       1 server_linux.go:66] "Using iptables proxy"
I0128 10:25:36.094031       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0128 10:25:36.094104       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0128 10:25:36.112132       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0128 10:25:36.112227       1 server_linux.go:170] "Using iptables Proxier"
I0128 10:25:36.114438       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0128 10:25:36.129184       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0128 10:25:36.141434       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0128 10:25:36.141516       1 server.go:497] "Version info" version="v1.32.0"
I0128 10:25:36.141523       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0128 10:25:36.160388       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0128 10:25:36.172604       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0128 10:25:36.173782       1 config.go:199] "Starting service config controller"
I0128 10:25:36.173819       1 shared_informer.go:313] Waiting for caches to sync for service config
I0128 10:25:36.173843       1 config.go:329] "Starting node config controller"
I0128 10:25:36.173854       1 shared_informer.go:313] Waiting for caches to sync for node config
I0128 10:25:36.173868       1 config.go:105] "Starting endpoint slice config controller"
I0128 10:25:36.174007       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0128 10:25:36.274607       1 shared_informer.go:320] Caches are synced for node config
I0128 10:25:36.274630       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0128 10:25:36.274646       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [ec9dbc749a2d] <==
I0128 10:08:16.308767       1 serving.go:386] Generated self-signed cert in-memory
W0128 10:08:18.123689       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0128 10:08:18.123720       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0128 10:08:18.123729       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0128 10:08:18.123733       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0128 10:08:18.315304       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0128 10:08:18.315332       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 10:08:18.317670       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0128 10:08:18.317695       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0128 10:08:18.317885       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0128 10:08:18.318200       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0128 10:08:18.419048       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0128 10:25:13.326983       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [f4fdb70e4290] <==
I0128 10:25:32.137429       1 serving.go:386] Generated self-signed cert in-memory
W0128 10:25:34.109652       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0128 10:25:34.110844       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0128 10:25:34.110865       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0128 10:25:34.110871       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0128 10:25:34.214883       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0128 10:25:34.214952       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0128 10:25:34.218248       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0128 10:25:34.218295       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0128 10:25:34.218373       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0128 10:25:34.218392       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0128 10:25:34.318660       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Jan 28 11:02:46 minikube kubelet[1631]: E0128 11:02:46.824723    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:02:46 minikube kubelet[1631]: E0128 11:02:46.826070    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:02:57 minikube kubelet[1631]: E0128 11:02:57.824985    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:02:57 minikube kubelet[1631]: E0128 11:02:57.826597    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:03:09 minikube kubelet[1631]: E0128 11:03:09.824687    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:03:09 minikube kubelet[1631]: E0128 11:03:09.825917    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:03:24 minikube kubelet[1631]: E0128 11:03:24.824949    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:03:24 minikube kubelet[1631]: E0128 11:03:24.828203    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:03:36 minikube kubelet[1631]: E0128 11:03:36.824979    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:03:36 minikube kubelet[1631]: E0128 11:03:36.826496    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:03:51 minikube kubelet[1631]: E0128 11:03:51.824267    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:03:51 minikube kubelet[1631]: E0128 11:03:51.825637    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:04:04 minikube kubelet[1631]: E0128 11:04:04.824337    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:04:04 minikube kubelet[1631]: E0128 11:04:04.826286    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:04:17 minikube kubelet[1631]: E0128 11:04:17.824641    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:04:17 minikube kubelet[1631]: E0128 11:04:17.825882    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:04:29 minikube kubelet[1631]: E0128 11:04:29.824136    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:04:29 minikube kubelet[1631]: E0128 11:04:29.825338    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:04:42 minikube kubelet[1631]: E0128 11:04:42.824911    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:04:42 minikube kubelet[1631]: E0128 11:04:42.826856    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:04:53 minikube kubelet[1631]: E0128 11:04:53.824542    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:04:53 minikube kubelet[1631]: E0128 11:04:53.825813    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:05:05 minikube kubelet[1631]: E0128 11:05:05.824831    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:05:05 minikube kubelet[1631]: E0128 11:05:05.826125    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:05:18 minikube kubelet[1631]: E0128 11:05:18.829465    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:05:18 minikube kubelet[1631]: E0128 11:05:18.830873    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:05:29 minikube kubelet[1631]: E0128 11:05:29.824939    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:05:29 minikube kubelet[1631]: E0128 11:05:29.826373    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:05:44 minikube kubelet[1631]: E0128 11:05:44.824421    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:05:44 minikube kubelet[1631]: E0128 11:05:44.826031    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:05:59 minikube kubelet[1631]: E0128 11:05:59.825012    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:05:59 minikube kubelet[1631]: E0128 11:05:59.826219    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:06:14 minikube kubelet[1631]: E0128 11:06:14.825585    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:06:14 minikube kubelet[1631]: E0128 11:06:14.827822    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:06:29 minikube kubelet[1631]: E0128 11:06:29.824188    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:06:29 minikube kubelet[1631]: E0128 11:06:29.825561    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:06:41 minikube kubelet[1631]: E0128 11:06:41.825233    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:06:41 minikube kubelet[1631]: E0128 11:06:41.826807    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:06:54 minikube kubelet[1631]: E0128 11:06:54.824061    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:06:54 minikube kubelet[1631]: E0128 11:06:54.825601    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:07:05 minikube kubelet[1631]: E0128 11:07:05.824794    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:07:05 minikube kubelet[1631]: E0128 11:07:05.826116    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:07:20 minikube kubelet[1631]: E0128 11:07:20.825594    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:07:20 minikube kubelet[1631]: E0128 11:07:20.827434    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:07:33 minikube kubelet[1631]: E0128 11:07:33.824385    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:07:33 minikube kubelet[1631]: E0128 11:07:33.825749    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:07:47 minikube kubelet[1631]: E0128 11:07:47.824906    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:07:47 minikube kubelet[1631]: E0128 11:07:47.826522    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:08:01 minikube kubelet[1631]: E0128 11:08:01.824902    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:08:01 minikube kubelet[1631]: E0128 11:08:01.826330    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:08:13 minikube kubelet[1631]: E0128 11:08:13.824608    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:08:13 minikube kubelet[1631]: E0128 11:08:13.825963    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:08:25 minikube kubelet[1631]: E0128 11:08:25.825145    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:08:25 minikube kubelet[1631]: E0128 11:08:25.826434    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:08:36 minikube kubelet[1631]: E0128 11:08:36.833767    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:08:36 minikube kubelet[1631]: E0128 11:08:36.835031    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:08:51 minikube kubelet[1631]: E0128 11:08:51.824919    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:08:51 minikube kubelet[1631]: E0128 11:08:51.826363    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"
Jan 28 11:09:03 minikube kubelet[1631]: E0128 11:09:03.824389    1631 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:nodejs-app,Image:sibichandran2001/node-app:latest\",Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqcwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nodejs-app-bbdff55d-mbc8k_default(d4113cb4-2809-4de9-9678-8f632d7a64bd): InvalidImageName: Failed to apply default image tag \"sibichandran2001/node-app:latest\\\"\": couldn't parse image name \"sibichandran2001/node-app:latest\\\"\": invalid reference format" logger="UnhandledError"
Jan 28 11:09:03 minikube kubelet[1631]: E0128 11:09:03.825707    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nodejs-app\" with InvalidImageName: \"Failed to apply default image tag \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": couldn't parse image name \\\"sibichandran2001/node-app:latest\\\\\\\"\\\": invalid reference format\"" pod="default/nodejs-app-bbdff55d-mbc8k" podUID="d4113cb4-2809-4de9-9678-8f632d7a64bd"


==> storage-provisioner [204c8c2002e4] <==
I0128 10:25:35.729851       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0128 10:26:05.735844       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [a27d223a200b] <==
I0128 10:26:18.953914       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0128 10:26:18.965449       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0128 10:26:18.965526       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0128 10:26:36.362013       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0128 10:26:36.362126       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_8286a8de-42d1-4b59-9a3f-c5838799cceb!
I0128 10:26:36.362375       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"a50c923d-d9bd-4ae8-9ef5-633f76518c3a", APIVersion:"v1", ResourceVersion:"2176", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_8286a8de-42d1-4b59-9a3f-c5838799cceb became leader
I0128 10:26:36.463372       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_8286a8de-42d1-4b59-9a3f-c5838799cceb!

